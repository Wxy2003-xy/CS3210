\documentclass{article}
\usepackage[utf8]{inputenc}    % For UTF-8 character encoding
\usepackage[T1]{fontenc}       % For proper font encoding
\usepackage{lmodern}           % Improved font rendering
\usepackage{amsmath}   % For advanced mathematical formatting
\usepackage{amssymb}   % For mathematical symbols
\usepackage{geometry}  % Adjust page margins
\usepackage{enumerate} % For custom lists
\usepackage{xcolor}  % for coloring
\usepackage{amsthm}
\usepackage{pdfpages}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\usepackage{listings}  % for code listings
\usepackage{tikz-cd}
\usepackage{forest}
\usepackage{booktabs}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{arrows.meta,decorations.pathreplacing,calc}
\lstset{frame=tb,
  language=C,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,   
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{brown},
  stringstyle=\color{orange},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}
\geometry{top=0.2in, bottom=0.2in, left=0.2in, right=0.2in}

\begin{document}
\title{CS3210}
\author{
  Wang Xiyu
}
\maketitle
\section{Computer architecture}
\subsection{Parallelism}
\begin{itemize}
    \item Concurrency:
    \begin{itemize}
        \item Multiple tasks can start, run and complete in overlapping time period
        \item may not be running or executing at the same instant
        \item multiple execution flows make progress at the same time by interleaving their execution OR by the same time
    \end{itemize} 
    \item Parallelism:
    \begin{itemize}
        \item Multiple tasks running simultaneosuly
        \item Not only making progress, but also execute simultaneously
    \end{itemize}
\end{itemize}

\begin{itemize}
    \item Single processer:
    \begin{itemize}
        \item Bit level paralleism:
        \begin{itemize}
            \item parallelism by increasing the processor word size, e.g. parallel addition of 64 bit numbers on 32 bit machine
        \end{itemize}
        \item Instruction level parallelism:
        \begin{itemize}
            \item pipelining: [time parallelism] number of pipeline stages = maximum achievable speedup
            \item superscaling: [space parallelism] Duplicate the pipeline, multiple instruction can be on the same excution stage. Scheduling is challenging. Stronger structural hazard, less cycles per instruction
        \end{itemize}
        \item Thread level parallelism:
        \begin{itemize}
            \item Simultaneous multithreading(SMT): processor provides hardware support for multiple thread level context
            \item hyper-threading: executing multiple threads per processor at the same time
        \end{itemize}
    \end{itemize}
    \item Multi-Processor:
    \begin{itemize}
        \item Shared memory:
        \item Distributed memory
    \end{itemize}
    \item Multicore processor archiecture
    \begin{itemize}
        \item hierarchical design:
        \begin{itemize}
            \item multiple cores share multiple caches
            \item cache size increases from leaves to root, as more cores share the same cache
            \item external memory shared by all cores
        \end{itemize}
        \item pipelined design
        \begin{itemize}
            \item data elements are processed by multiple execution cores in a piplined way
            \item useful for sequential data element processing
        \end{itemize}
        \item network-based design
        \begin{itemize}
            \item cores and local caches and memory are connected via interconnection network
            \begin{itemize}
                \item Efficient on-chip interconnection: enough bandwidth, scalable
            \end{itemize}
        \end{itemize}
    \end{itemize}
    Multiprocessing vs. multithreading:
    \begin{itemize}
        \item Multiprocessing: high overhead, i.e. context switches; able to utilize multiple processing units
        \item Multthreading: low overhead; but effectively utilising the same processing unit
    \end{itemize}
\end{itemize}



% -------- Table 1: Processor vs Core vs Processing Unit vs Logical Core --------
\begin{table}[h]
    \centering
    \caption{Distinguishing “processor”, “core”, “processing unit”, and “logical core”}
    \small
    \begin{tabular}{@{}p{2.9cm}p{3.2cm}p{3.8cm}p{3.2cm}p{4.2cm}@{}}
    \toprule
    \textbf{Term} & \textbf{What it is} & \textbf{Independence / Context} & \textbf{Shares With} & \textbf{OS Sees / Notes} \\
    \midrule
    Processor (CPU / package) &
    Physical chip/package containing compute resources (cores, caches, I/O, memory controller). &
    Multiple cores; each core is an independent execution engine. &
    All cores share off-core resources (e.g., memory controller, sometimes LLC). &
    OS sees one processor package with $N$ cores; socket count in NUMA systems. \\
    \addlinespace
    Core (physical core) &
    An independent execution pipeline (fetch/decode/execute). &
    Can run its own instruction stream; has private L1/L2 (often). &
    Shares last-level cache (LLC) and memory controller with other cores on the processor. &
    OS schedules threads to cores; true parallelism across cores. \\
    \addlinespace
    Processing unit (execution context) &
    Generic term for a hardware context that can run instructions (a core \emph{or} a hardware thread). &
    Independent program counter, registers, and a stack. &
    If it is a hardware thread, shares core pipelines with sibling threads. &
    Ambiguous in literature; often equals “schedulable hardware context”. \\
    \addlinespace
    Logical core (hardware thread, SMT/HT) &
    A virtualized execution context exposed by SMT/Hyper-Threading. &
    Own PC/regs/stack, but \emph{shares} core’s execution units and caches. &
    Siblings on the same physical core compete for pipelines, cache ports, bandwidth. &
    OS treats each logical core as a CPU; throughput gain depends on resource contention and ILP. \\
    \bottomrule
    \end{tabular}
    \end{table}

% -------- Table 2: Flynn’s Taxonomy --------
\begin{table}[h]
    \centering
    \caption{Flynn’s taxonomy: instruction/data streams, examples, and caveats}
    \small
    \begin{tabular}{@{}p{1.6cm}p{2.2cm}p{2.2cm}p{5.2cm}p{5.2cm}@{}}
    \toprule
    \textbf{Class} & \textbf{Instr. Streams} & \textbf{Data Streams} & \textbf{Typical Examples} & \textbf{Notes / Caveats} \\
    \midrule
    SISD &
    1 & 1 &
    Classic single-core CPUs; pipelined or superscalar scalar execution. &
    Pipelining/superscalar improve throughput but pipelining does not add new data streams, superscalar does not add new instruction streams. \\
    \addlinespace
    SIMD &
    1 & Many &
    Vector/SIMD ISAs (SSE/AVX/NEON), GPU warp/warp-lane execution, vector processors. &
    Same instruction applied to multiple data elements in lock-step; divergence harms efficiency (masks). \\
    \addlinespace
    MISD &
    Many & 1 &
    Rare/mostly theoretical; certain fault-tolerant or systolic designs sometimes cited. &
    Not common in general-purpose computing; examples are niche/controversial. \\
    \addlinespace
    MIMD &
    Many & Many &
    Multicore/multiprocessor systems, clusters; CPUs with SMT (each HW thread its own stream). &
    Threads/processes can execute different code on different data; includes shared-memory and distributed models. \\
    \addlinespace
    \multicolumn{5}{@{}p{16.8cm}@{}}{\emph{Hybrids:} 
    Modern systems often combine MIMD (many cores/threads) with SIMD (per-core vectors). A single program may be MIMD\,+\,SIMD (e.g., OpenMP across cores + AVX within each core; GPUs: MIMD across warps/SMs, SIMD within a warp).} \\
    \bottomrule
    \end{tabular}
\end{table}
    

\subsection{Memory Organization}
Parallel computers:
\begin{itemize}
    \item Distibuted-memory: Multiple computers
    \begin{itemize}
        \item each node is an indenpendent unit, with processor, memory etc. 
        \item physically distributed memory modules, memory local and private to each node
    \end{itemize}
    \item Shared-memory: multiprocessor: programs and threads access memory through shared memory provider, unaware of the acual hardware memory architecture, requires cache coherence and memory consistency.
    \begin{itemize}
        \item cache coherence: local update by one processing unit, other PU should see the change being reflected in their copy of the same data in their cache
        \item memory consistency
    \end{itemize}
    Shared memory model:
    \begin{itemize}
        \item uniform memory access(UMA): same latency of accessing the main memory for all processors. Contention makes this unsuitable for large number of processors. 
        \item Non-uniform memory access(NUMA)
        \item Cache-coherent Non-uniform memory access(ccNUMA)
        \item Cache-only memory access(COMA)
    \end{itemize}
    \item Hybrid(distributed shared memory)
\end{itemize}
\begin{itemize}
    \item Latency: time taken for a request from the processor to be serviced by the memory
    \item Bandwidth: the rate at which the memory system can provide data to the processor
    \item stall: When the processor cannot run the next instruction in an instruction stream due to dependency on a previous instrucion 
\end{itemize}
\subsection{Data and task parallelism}
\begin{itemize}
    \item Data parallelism: multiple processing units carry out similar task on different part of data eg SIMD, SPMD in MIMD
    \item Task/functional parallelism: partition the task to solve among different PUs
\end{itemize}
\subsection{Task dependency and degree of concurrency}
Task dependency graph is a directed acyclic graph, nodes represent tasks with its execution time as value. This represents the control
dependency between the tasks. 
\begin{itemize}
    \item critical path: the longest path 
    \item degree of concurrency: $\frac{\sum \text{work}}{\text{critical path length}}$, an  indication of the amount of work that can be done in concurrence
\end{itemize}
\subsection{IPC models}
\begin{itemize}
    \item Shared address space: 
    \begin{itemize}
        \item communication abstraction:
        \begin{itemize}
            \item Tasks communicate by reading and writing to shared variables
            \item use locks to ensure mutua exclusion
            \item logical extension of single processor programming
            \item require hardware support: able to share address space, ie shared memory systems
            \item hard to scale due to memroy contention when multiple PUs accessing the same shared address space
            \item can be mimiced on systems without required hardware support, implemented by message passing: 
            \begin{itemize}
                \item write to shared variables: send message to invalidate all pages containing shared variables
                \item read a shared variable: page fault handler issues appropriate network requests 
                \item pure software solution, but inefficient
            \end{itemize}
            \item very little structure
            \item all threads can read and write to all shared variables
            \item Not all reads and writes have the same cost (NUMA), and the cost is not apparent in program text
        \end{itemize}
        \item Data parallel
        \begin{itemize}
            \item SIMD, vector processors
            \item Basic structure: map a function onto a collection of data
            \item side effect free
            \item no communication among distinct function invocations
            \item Stream programming model
            \item very rigid computation structure
        \end{itemize}
    \end{itemize}
    \item Message passing 
    \begin{itemize}
        \item tasks communicate via explicit sending and receiving messages
        \item no need for system wide load and store implementation supported by hardwares
        \item matches distributed memory system where theres no physically possible shared address space
        \item more costly than shared address space model
        \item Possible and common to implement message passing abstraction on shared memory machines:
        \begin{itemize}
            \item sending message: copy date into message library buffer
            \item receiving message: copy data from message library buffer 
        \end{itemize}
        \item higly structured, all communication occurs in the form of messages
    \end{itemize}
\end{itemize}
\subsection{Program Parallelization}
The transformation of sequential program into parallel programs
\begin{itemize}
    \item Fine-grain: a sequence of instructions
    \item in between: a sequecne of statements 
    \item coarse-grain: a function 
\end{itemize}

Steps to parallelize a program: (Foster's methodology)
\begin{enumerate}
    \item Partitioning: break down problem into many smaller pieces
    \begin{itemize}
        \item Data centric: domain decomposition: divide data into pieces of similar size, and determine how to associate computatons with the data (Data parallelism)
        \item Computation centric: Functional decompositon: determine how to associate data with the conputations (Task parallelism)
        \item Rules of thumbs:
        \begin{itemize}
            \item At least 10x more primitive tasks than cores 
            \item minimize redundant computation and data storage
            \item primitive tasks should have roughly the same size
            \item number of tasks is an increasing function of the problem size
        \end{itemize}
    \end{itemize}
    \item Communicaton: provide data required by the partitioned tasks
    \begin{itemize}
        \item Tasks are intended to be executed in parallel, but may not be indenpently executed, thus need to determine data passed among tasks
        \begin{itemize}
            \item Local communicaton
            \begin{itemize}
                \item Task needs data from a small number of other tasks (neighbours)
                \item create channels illustrating data flow
            \end{itemize}
            \item Global communication
            \begin{itemize}
                \item significant number of tasks contribute data to perform a computation 
                \item no need to create channels for communication early in design
            \end{itemize}
            \item Rules of thumb:
            \begin{itemize}
                \item Communication operations are balanced aamong tasks
                \item tasks commnunication with only a small group of neighbours
                \item commincation can be performed in parallel
                \item Ideally, distribute and overlap computation and commincation 
            \end{itemize}
        \end{itemize}
    \end{itemize}
    \item Agglomeration: decrease communication and develpent cost, while maintaining flexibility
    \begin{itemize}
        \item eliminate communication between primitive tasks by agglomerating into consolidated tasks
        \item Ideally combine groups of sending and receiving tasks 
        \begin{itemize}
            \item increase locality of parallel program
            \item ensure the number of tasks increase with problem size 
            \item ensure the number of consoliated tasks is suitable for the target system
            \item balance agglomeration and code modification cost
        \end{itemize}
    \end{itemize}
    \item Mapping: make tasks to processors, goal: minimize execution time
    \begin{itemize}
        \item Maximize processor utilization
        \item minimize inter-processor communication
        \item Rules of thumb:
        \begin{itemize}
            \item NP-hard in general, but can rely on heuristics
            \item evaluate static and dynamic task allocation
            \begin{itemize}
                \item If use dynamic tasks allocation, the task allocator should not be a bottleneck
                \item if use static task allocation, the ratio of tasks to cores is at least 10:1
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{enumerate}
\subsection{Parallel programming patterns}
\subsubsection*{Fork-join}
\subsubsection*{Parbegin-Parend}
Suitable for openMP implementation
\subsubsection*{SPMD and SIMD}
\subsubsection*{Master-worker}
\subsubsection*{Task pool}
Suitable for heterogeneous jobs, eg when the size of incoming problems is unpredicatbale
\subsubsection*{Producer-consumer}
\subsubsection*{Pipelining}
Suitable for task parallelism, when different partition of the task are roughly the same size. Uneven partition size renders pipelining or 
task parallelism inefficient 
uuu
\section{Performance}
Goal: reduce response time: wall clock time
\subsection{sequential}

\begin{itemize}
    \item User CPU time: for user prog
    \item System CPU time: OS routines: depends on the OS implementatuon 
    \item waiting time: I/O and other porgrames (due to time sharing, other programs get hihger piroirty etc): depends on the load o the coputer system (for seq program this shouldnt be high)
\end{itemize}
\[Time_{user}(A) =\left( N_{cycle}(A) + N_{mm\_cycle}(A)\right)\times Time_{cycle}\]
User CPU time of prog A = number of CPU cycles needed fo rall instr times cycle time of CPU $\frac{1}{\text{clock rate}}$\\
instructions have different exe time: 
\[N_{cycle}(A) = \sum_{i=1}^{n}n_i(A)\times CPI\]
\subsubsection*{Memory access}
\[ N_{mm\_cycle}(A) = N_{read}(A) + N_{write}(A) = \]\[N_{read\_cycle}(A) \times R_{read_miss}(A) \times N_{read\_miss\_cycle}(A + 
N_{write\_cycle}(A) \times R_{write_miss}(A) \times N_{write\_miss\_cycle}(A))\]
\[Time_{user}(A) = (N_{instr}(A) \times CPI(A) + N_{rw\_op}(A) \times R_{miss}(A) \times N_{miss\_cycles})\times Time_{cycle}\]
k level cache mis:
\[R_{miss\_global} = \prod_{i=1}^{k} R^{Lk}_{read\_miss}(A)\]
CPI depends on the internal organization of the CPU, memory system and compiler; $N_{instr}$ depends on the compiler and ISA\\
\subsubsection*{Throughput: Million Instruction Per Second (MIPS) Not a good indicator}
\[MIPS(A) = \frac{N_{instr}(A)}{Time_{user}(A) \times 10^6} = \frac{clock\_frequency}{CPI(A) \times 10^6}\]
\subsubsection*{Throughput: Million Floating Operations Per Second}
\[MFLOPS = \frac{N_{fl\_op}(A)}{Time_{user}(A) \times 10^6}\]
issues: 
\begin{itemize}
    \item no differentisation between different types of fp op 
    \item specfic goal: do fp op
\end{itemize}
\subsection{parallel}
$T_{processor}(n)$ depends on:
\begin{itemize}
    \item time for local computatio s
    \item time for exchanged of data between PUs
    \item time for synchronizaiton between PUs
    \item waititng time: unbalanced load; memory contention; synchronization eg waiting to access shared data structure
\end{itemize}

Speed up
\[S_p(n) = \frac{T_{best\_seq}(n)}{T_p(n)}\]
Theoratically $S_p(n) \leq p$, but in practice, superlinear speedup is possile, eg: problem working task fits in the cache, and one threads pulls data into L3 and other threads can use\\
Cost
\[C_p(n) = p \times T_p{n}\]
A parallel program is cost-optimal if it exe the same numbr of insructions as the best sequntial program\\
Effiiency
\[E_p(n) = \frac{T_{best\_seq}(n)}{C_p(n)} = \frac{S_p(n)}{p} = \frac{T_{best\_seq}(n)}{p\times T_p(n)}\]
Ideally, $S_p(n) = p \rightarrow E_p(n) = 1$\\
Issues: 
\begin{itemize}
    \item Best sequential algorithm may not be known
    \item asymptotically optimal algorithm may not be the fastest in practice
\end{itemize}
\subsection{Scalability}
\subsubsection*{Amdahl's law: }
speed up is limited by the fraction of the algo that cannot be parallelized\\
\[T = fT_*(n) + (1-f)T_*(n)\]
\[S_p = \frac{T_*(n)}{fT_*(n) + \frac{(1-f)}{p}\times T_*(n)} = \frac{1}{f + \frac{1-f}{p}} \leq \frac{1}{f}\]
implication: need better compilers to reduce f\\
drawback: f is not constant wrt n\\
\subsubsection*{Gustafson's law:}
\[\lim_{n\rightarrow \infty} f(n) = 0 \rightarrow \lim_{n\rightarrow \infty} S_p(n) = \frac{p}{1 + (p-1)f(n)} = p\]
Implication: Amdahl's law can be circumvented for large size problems
\begin{itemize}
    \item 
    \item 
\end{itemize}

The interaction between size of th eproble  and the size of the parallel machine, dependedt on application, 


\subsubsection*{Arithmetic Intensity}
\[\text{Arithmetic intensity} = \frac{\text{the amount of computation}}{\text{the amount of communication}}\]




\subsubsection*{Contentiion}
many requests to a resource are made witin a small window of time, resource hotspot. Use tree structure communcation to reduce contention, at the cost of high latency 
\subsubsection*{Locality}
Exploit sharing: co locate tasks that operate on the same data, 








\section{GPGPU}

\end{document}