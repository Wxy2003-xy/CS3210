\documentclass{article}
\usepackage[utf8]{inputenc}    % For UTF-8 character encoding
\usepackage[T1]{fontenc}       % For proper font encoding
\usepackage{lmodern}           % Improved font rendering
\usepackage{amsmath}   % For advanced mathematical formatting
\usepackage{amssymb}   % For mathematical symbols
\usepackage{geometry}  % Adjust page margins
\usepackage{enumerate} % For custom lists
\usepackage{xcolor}  % for coloring
\usepackage{amsthm}
\usepackage{pdfpages}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\usepackage{listings}  % for code listings
\usepackage{tikz-cd}
\usepackage{forest}
\usepackage{booktabs}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{arrows.meta,decorations.pathreplacing,calc}
\lstset{frame=tb,
  language=C,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,   
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{brown},
  stringstyle=\color{orange},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}
\geometry{top=0.2in, bottom=0.2in, left=0.2in, right=0.2in}

\begin{document}
\title{CS3210}
\author{
  Wang Xiyu
}
\maketitle
\section{Computer architecture}
\subsection{Parallelism}
\begin{itemize}
    \item Concurrency:
    \begin{itemize}
        \item Multiple tasks can start, run and complete in overlapping time period
        \item may not be running or executing at the same instant
        \item multiple execution flows make progress at the same time by interleaving their execution OR by the same time
    \end{itemize} 
    \item Parallelism:
    \begin{itemize}
        \item Multiple tasks running simultaneosuly
        \item Not only making progress, but also execute simultaneously
    \end{itemize}
\end{itemize}

\begin{itemize}
    \item Single processer:
    \begin{itemize}
        \item Bit level paralleism:
        \begin{itemize}
            \item parallelism by increasing the processor word size, e.g. parallel addition of 64 bit numbers on 32 bit machine
        \end{itemize}
        \item Instruction level parallelism:
        \begin{itemize}
            \item pipelining: [time parallelism] number of pipeline stages = maximum achievable speedup
            \item superscaling: [space parallelism] Duplicate the pipeline, multiple instruction can be on the same excution stage. Scheduling is challenging. Stronger structural hazard, less cycles per instruction
        \end{itemize}
        \item Thread level parallelism:
        \begin{itemize}
            \item Simultaneous multithreading(SMT): processor provides hardware support for multiple thread level context
            \item hyper-threading: executing multiple threads per processor at the same time
        \end{itemize}
    \end{itemize}
    \item Multi-Processor:
    \begin{itemize}
        \item Shared memory:
        \item Distributed memory
    \end{itemize}
    \item Multicore processor archiecture
    \begin{itemize}
        \item hierarchical design:
        \begin{itemize}
            \item multiple cores share multiple caches
            \item cache size increases from leaves to root, as more cores share the same cache
            \item external memory shared by all cores
        \end{itemize}
        \item pipelined design
        \begin{itemize}
            \item data elements are processed by multiple execution cores in a piplined way
            \item useful for sequential data element processing
        \end{itemize}
        \item network-based design
        \begin{itemize}
            \item cores and local caches and memory are connected via interconnection network
            \begin{itemize}
                \item Efficient on-chip interconnection: enough bandwidth, scalable
            \end{itemize}
        \end{itemize}
    \end{itemize}
    Multiprocessing vs. multithreading:
    \begin{itemize}
        \item Multiprocessing: high overhead, i.e. context switches; able to utilize multiple processing units
        \item Multthreading: low overhead; but effectively utilising the same processing unit
    \end{itemize}
\end{itemize}



% -------- Table 1: Processor vs Core vs Processing Unit vs Logical Core --------
\begin{table}[h]
    \centering
    \caption{Distinguishing “processor”, “core”, “processing unit”, and “logical core”}
    \small
    \begin{tabular}{@{}p{2.9cm}p{3.2cm}p{3.8cm}p{3.2cm}p{4.2cm}@{}}
    \toprule
    \textbf{Term} & \textbf{What it is} & \textbf{Independence / Context} & \textbf{Shares With} & \textbf{OS Sees / Notes} \\
    \midrule
    Processor (CPU / package) &
    Physical chip/package containing compute resources (cores, caches, I/O, memory controller). &
    Multiple cores; each core is an independent execution engine. &
    All cores share off-core resources (e.g., memory controller, sometimes LLC). &
    OS sees one processor package with $N$ cores; socket count in NUMA systems. \\
    \addlinespace
    Core (physical core) &
    An independent execution pipeline (fetch/decode/execute). &
    Can run its own instruction stream; has private L1/L2 (often). &
    Shares last-level cache (LLC) and memory controller with other cores on the processor. &
    OS schedules threads to cores; true parallelism across cores. \\
    \addlinespace
    Processing unit (execution context) &
    Generic term for a hardware context that can run instructions (a core \emph{or} a hardware thread). &
    Independent program counter, registers, and a stack. &
    If it is a hardware thread, shares core pipelines with sibling threads. &
    Ambiguous in literature; often equals “schedulable hardware context”. \\
    \addlinespace
    Logical core (hardware thread, SMT/HT) &
    A virtualized execution context exposed by SMT/Hyper-Threading. &
    Own PC/regs/stack, but \emph{shares} core’s execution units and caches. &
    Siblings on the same physical core compete for pipelines, cache ports, bandwidth. &
    OS treats each logical core as a CPU; throughput gain depends on resource contention and ILP. \\
    \bottomrule
    \end{tabular}
    \end{table}

% -------- Table 2: Flynn’s Taxonomy --------
\begin{table}[h]
    \centering
    \caption{Flynn’s taxonomy: instruction/data streams, examples, and caveats}
    \small
    \begin{tabular}{@{}p{1.6cm}p{2.2cm}p{2.2cm}p{5.2cm}p{5.2cm}@{}}
    \toprule
    \textbf{Class} & \textbf{Instr. Streams} & \textbf{Data Streams} & \textbf{Typical Examples} & \textbf{Notes / Caveats} \\
    \midrule
    SISD &
    1 & 1 &
    Classic single-core CPUs; pipelined or superscalar scalar execution. &
    Pipelining/superscalar improve throughput but pipelining does not add new data streams, superscalar does not add new instruction streams. \\
    \addlinespace
    SIMD &
    1 & Many &
    Vector/SIMD ISAs (SSE/AVX/NEON), GPU warp/warp-lane execution, vector processors. &
    Same instruction applied to multiple data elements in lock-step; divergence harms efficiency (masks). \\
    \addlinespace
    MISD &
    Many & 1 &
    Rare/mostly theoretical; certain fault-tolerant or systolic designs sometimes cited. &
    Not common in general-purpose computing; examples are niche/controversial. \\
    \addlinespace
    MIMD &
    Many & Many &
    Multicore/multiprocessor systems, clusters; CPUs with SMT (each HW thread its own stream). &
    Threads/processes can execute different code on different data; includes shared-memory and distributed models. \\
    \addlinespace
    \multicolumn{5}{@{}p{16.8cm}@{}}{\emph{Hybrids:} 
    Modern systems often combine MIMD (many cores/threads) with SIMD (per-core vectors). A single program may be MIMD\,+\,SIMD (e.g., OpenMP across cores + AVX within each core; GPUs: MIMD across warps/SMs, SIMD within a warp).} \\
    \bottomrule
    \end{tabular}
\end{table}
    

\subsection{Memory Organization}
Parallel computers:
\begin{itemize}
    \item Distibuted-memory: Multiple computers
    \begin{itemize}
        \item each node is an indenpendent unit, with processor, memory etc. 
        \item physically distributed memory modules, memory local and private to each node
    \end{itemize}
    \item Shared-memory: multiprocessor: programs and threads access memory through shared memory provider, unaware of the acual hardware memory architecture, requires cache coherence and memory consistency.
    \begin{itemize}
        \item cache coherence: local update by one processing unit, other PU should see the change being reflected in their copy of the same data in their cache
        \item memory consistency
    \end{itemize}
    Shared memory model:
    \begin{itemize}
        \item uniform memory access(UMA): same latency of accessing the main memory for all processors. Contention makes this unsuitable for large number of processors. 
        \item Non-uniform memory access(NUMA)
        \item Cache-coherent Non-uniform memory access(ccNUMA)
        \item Cache-only memory access(COMA)
    \end{itemize}
    \item Hybrid(distributed shared memory)
\end{itemize}
\begin{itemize}
    \item Latency: time taken for a request from the processor to be serviced by the memory
    \item Bandwidth: the rate at which the memory system can provide data to the processor
    \item stall: When the processor cannot run the next instruction in an instruction stream due to dependency on a previous instrucion 
\end{itemize}

\section{Performance}
Goal: reduce response time: wall clock time

\subsection{sequential}

\begin{itemize}
    \item User CPU time: for user prog
    \item System CPU time: OS routines: depends on the OS implementatuon 
    \item waiting time: I/O and other porgrames (due to time sharing, other programs get hihger piroirty etc): depends on the load o the coputer system (for seq program this shouldnt be high)
\end{itemize}
\[Time_{user}(A) =\left( N_{cycle}(A) + N_{mm\_cycle}(A)\right)\times Time_{cycle}\]
User CPU time of prog A = number of CPU cycles needed fo rall instr times cycle time of CPU $\frac{1}{\text{clock rate}}$\\
instructions have different exe time: 
\[N_{cycle}(A) = \sum_{i=1}^{n}n_i(A)\times CPI\]
\[ N_{mm\_cycle}(A) = N_{read}(A) + N_{write}(A) = \]\[N_{read\_cycle}(A) \times R_{read_miss}(A) \times N_{read\_miss\_cycle}(A + 
N_{write\_cycle}(A) \times R_{write_miss}(A) \times N_{write\_miss\_cycle}(A))\]
\[Time_{user}(A) = N_{instr}(A) \times CPI(A) \times Time_{cycle}\]
CPI depends on the internal organization of the CPU, memory system and compiler; $N_{instr}$ depends on the compiler and ISA\\
\subsubsection*{Memory access}

\[MFLOPS = \frac{N_{fi\_ops}(A)}{Time_{user}(A)\time 10^6}\]

\subsection{parallel}
\[T_{processor}(n)\]
\begin{itemize}
    \item time for local computatio s
    \item time for exchanged of data between PUs
    \item time for synchronizaiton between PUs
    \item waititng time: unbalanced load; memory contention; synchronization eg waiting to access shared data structure
\end{itemize}

Speed up
\[S_p(n) = \frac{T_{best\_seq}(n)}{T_p(n)}\]
Theoratically $S_p(n) \leq p$, but in practice, superlinear speedup is possile, eg: problem working task fits in the cache, and one threads pulls data into L3 and other threads can use\\
Cost
\[C_p(n) = p \times T_p{n}\]
A parallel program is cost-optimal if it exe the same numbr of insructions as the best sequntial program\\
Effiiency
\[E_p(n) = \frac{T_{best\_seq}(n)}{C_p(n)} = \frac{S_p(n)}{p} = \frac{T_{best\_seq}(n)}{p\times T_p(n)}\]
Ideally, $S_p(n) = p \rightarrow E_p(n) = 1$


\subsection{Scalability}
\subsubsection*{Amdahl's law: }
speed up is limited by the fraction of the algo that cannot be parallelized\\
\[T = fT_*(n) + (1-f)T_*(n)\]
\[S_p = \frac{T_*(n)}{fT_*(n) + \frac{(1-f)}{p}\times T_*(n)} = \frac{1}{f + \frac{1-f}{p}} \leq \frac{1}{f}\]
implication: need better compilers to reduce f\\
drawback: f is not constant wrt n\\
\subsubsection*{Gustafson's law:}
\[\lim_{n\rightarrow \infty} f(n) = 0 \rightarrow \lim_{n\rightarrow \infty} S_p(n) = p\]
\begin{itemize}
    \item 
    \item 
\end{itemize}

The interaction between size of th eproble  and the size of the parallel machine, dependedt on application, 







\subsubsection*{Contentiion}
many requests to a resource are made witin a small window of time, resource hotspot. Use tree structure communcation to reduce contention, at the cost of high latency 
\subsubsection*{Locality}
Exploit sharing: co locate tasks that operate on the same data, 
\end{document}